{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "de84dda4",
   "metadata": {},
   "source": [
    "## Homework 1: Applied Machine Learning"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b7101150",
   "metadata": {},
   "source": [
    "This assignment covers contents of the first three lectures. \n",
    "\n",
    "The emphasis for this assignment would be on the following:\n",
    "1. Data Visualization and Analysis\n",
    "2. Linear Models for Regression and Classification\n",
    "3. Support Vector Machines"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "49cffd03",
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "\n",
    "def fxn():\n",
    "    warnings.warn(\"deprecated\", DeprecationWarning)\n",
    "\n",
    "with warnings.catch_warnings():\n",
    "    warnings.simplefilter(\"ignore\")\n",
    "    fxn()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "96086fb6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "from numpy.linalg import inv\n",
    "%matplotlib inline\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler, OneHotEncoder, OrdinalEncoder\n",
    "from sklearn.metrics import r2_score\n",
    "from sklearn.svm import LinearSVC, SVC"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "965fbeb7",
   "metadata": {},
   "source": [
    "## Part 1: Data Visualization and Analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0b47876b",
   "metadata": {},
   "source": [
    "> \"Visualization gives you answers to questions you didn’t know you had.\" ~ Ben Schneiderman\n",
    ">\n",
    "\n",
    "Data visualization comes in handy when we want to understand data characteristics and read patterns in datasets with thousands of samples and features.\n",
    "\n",
    "<b>Note: Remember to label plot axes while plotting.</b>\n",
    "\n",
    "### The dataset to be used for this section is car_price.csv.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d7e0ed9a",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Load the dataset\n",
    "car_price_df = pd.read_csv('car_price.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3b2eb791",
   "metadata": {},
   "source": [
    "<b> 1.1 Plot the distribution of the following features as a small multiple of histograms. </b>\n",
    "1. carlength \n",
    "2. carwidth\n",
    "3. stroke \n",
    "4. curbweight\n",
    "\n",
    "<b></b>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "07a5016e",
   "metadata": {},
   "outputs": [],
   "source": [
    "### Code here"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e3a644ad",
   "metadata": {},
   "source": [
    "<b> 1.2 Plot a small multiple of bar charts to understand data distribution of the following categorical variables </b>\n",
    "1. fueltype \n",
    "2. drivewheel\n",
    "3. enginelocation\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "9dbf14aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "### Code here"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5ba00d88",
   "metadata": {},
   "source": [
    "<b> 1.3 Plot relationships between the following features and the target variable <em>price</em> as a small multiple of boxplots. </b>\n",
    "1. cylindernumber \n",
    "2. enginetype\n",
    "\n",
    "<b> Note: Make sure to order the x-axis labels in increasing order for cylindernumber. </b>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "10972818",
   "metadata": {},
   "outputs": [],
   "source": [
    "### Code here"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9a0d1501",
   "metadata": {},
   "source": [
    "<b> 1.4 What do you infer from the visualization above. Comment on the skewness of the distributions (histograms), class imbalance (bar charts), and relationship between categories and price of the car (boxplots). </b>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "3be9819e",
   "metadata": {},
   "outputs": [],
   "source": [
    "#### Comment here"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9239fbb2",
   "metadata": {},
   "source": [
    "------------------------------------------------------------------------------------------------------------------"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3367d474",
   "metadata": {},
   "source": [
    "## Part 2: Linear Models for Regression and Classification\n",
    "\n",
    "In this section, we will be implementing three linear models **linear regression, logistic regression, and SVM**. We will see that despite some of their differences at the surface, these linear models (and many machine learning models in general) are fundamentally doing the same thing - that is, optimizing model parameters to minimize a loss function on data."
   ]
  },
  {
   "attachments": {
    "images.jpg": {
     "image/jpeg": "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"
    }
   },
   "cell_type": "markdown",
   "id": "cab447e3",
   "metadata": {},
   "source": [
    "### 2.1 Linear Regression\n",
    "![images.jpg](attachment:images.jpg)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "45bef189",
   "metadata": {},
   "source": [
    "In part 1, we will use two datasets - synthetic and Car Price to train and evaluate our linear regression model.\n",
    "\n",
    "### Synthetic Data\n",
    "\n",
    "<b>2.1.1 Generate 100 samples of synthetic data using the following equations. </b>\n",
    "\n",
    "$ \\epsilon ∼ \\mathcal{N}(0,4) $\n",
    "\n",
    "$ y = 7x - 8 + \\epsilon $\n",
    "\n",
    "You may use [np.random.normal()](https://numpy.org/doc/stable/reference/random/generated/numpy.random.normal.html) for generating $\\epsilon$. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10239bb5",
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.seed(0)\n",
    "X = np.linspace(0, 15, 100)\n",
    "epsilon = ### Code here\n",
    "y = ### Code here"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b9e4653a",
   "metadata": {},
   "source": [
    "To apply linear regression, we need to first check if the assumptions of linear regression are not violated.\n",
    "\n",
    "Assumptions of Linear Regression:\n",
    "\n",
    "*   Linearity:  is a linear (technically affine) function of $x$.\n",
    "*   Independence: the $x$'s are independently drawn, and not dependent on each other.\n",
    "*   Homoscedasticity: the $\\epsilon$'s, and thus the $y$'s, have constant variance.\n",
    "*   Normality: the $\\epsilon$'s are drawn from a Normal distribution (i.e. Normally-distributed errors)\n",
    "\n",
    "These properties, as well as the simplicity of this dataset, will make it a good test case to check if our linear regression model is working properly."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "792aa17e",
   "metadata": {},
   "source": [
    "**2.1.2 Plot y vs X in the synthetic dataset as a scatter plot. Label your axes and make sure your y-axis starts from 0. Do the features have linear relationship?**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "a336a551",
   "metadata": {},
   "outputs": [],
   "source": [
    "### Code here"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "4b8ecd43",
   "metadata": {},
   "outputs": [],
   "source": [
    "#### Comment here"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3a11537d",
   "metadata": {},
   "source": [
    "### Car Price Prediction Dataset\n",
    "\n",
    "The objective of this dataset is to predict the price of a car based on its characterisitics. We will use linear regression to predict the price using its features. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c24099b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# split data into features and labels\n",
    "car_price_X = car_price_df.drop(columns=['price'])\n",
    "car_price_y = car_price_df['price']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0d3a72d0",
   "metadata": {},
   "source": [
    "**2.1.3 Plot the relationships between the label (price) and the continuous features (citympg, highwaympg, enginesize, horsepower) using a small multiple of scatter plots. Make sure to label the axes.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "152bfb7c",
   "metadata": {},
   "outputs": [],
   "source": [
    "### Code here"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a5d505b8",
   "metadata": {},
   "source": [
    "**2.1.4 From the visualizations above, do you think linear regression is a good model for this problem? Why and/or why not? Please explain.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "14ee2221",
   "metadata": {},
   "outputs": [],
   "source": [
    "#### Comment here"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "68fa093e",
   "metadata": {},
   "source": [
    "### Data Preprocessing\n",
    "\n",
    "Before we can fit a linear regression model, there are several pre-processing steps we should apply to the datasets:\n",
    "\n",
    "1. Encode categorial features appropriately.\n",
    "2. Remove highly collinear features by reading the correlation plot.\n",
    "3. Split the dataset into training (60%), validation (20%), and test (20%) sets.\n",
    "4. Standardize the columns in the feature matrices X_train, X_val, and X_test to have zero mean and unit variance. To avoid information leakage, learn the standardization parameters (mean, variance) from X_train, and apply it to X_train, X_val, and X_test.\n",
    "5. Add a column of ones to the feature matrices X_train, X_val, and X_test. This is a common trick so that we can learn a coefficient for the bias term of a linear model.\n",
    "\n",
    "The processing steps on the synthetic dataset have been provided for you below as a reference:\n",
    "\n",
    "**Note: Generate the synthetic data before running the next cell to avoid errors.**\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "8492d5af",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 1.          0.53651502]\n",
      " [ 1.         -1.00836082]\n",
      " [ 1.         -0.72094206]\n",
      " [ 1.         -0.25388657]\n",
      " [ 1.          0.64429705]] \n",
      "\n",
      " [55.47920661 13.42527931 26.39143796 36.62805794 65.38959977]\n"
     ]
    }
   ],
   "source": [
    "X = X.reshape((100, 1))   # Turn the X vector into a feature matrix X\n",
    "\n",
    "# 1. No categorical features in the synthetic dataset (skip this step)\n",
    "\n",
    "# 2. Only one feature vector\n",
    "\n",
    "# 3. Split the dataset into training (60%), validation (20%), and test (20%) sets\n",
    "X_dev, X_test, y_dev, y_test = train_test_split(X, y, test_size=0.2, random_state=0)\n",
    "X_train, X_val, y_train, y_val = train_test_split(X_dev, y_dev, test_size=0.25, random_state=0)\n",
    "\n",
    "# 4. Standardize the columns in the feature matrices\n",
    "scaler = StandardScaler()\n",
    "X_train = scaler.fit_transform(X_train)   # Fit and transform scalar on X_train\n",
    "X_val = scaler.transform(X_val)           # Transform X_val\n",
    "X_test = scaler.transform(X_test)         # Transform X_test\n",
    "\n",
    "# 5. Add a column of ones to the feature matrices\n",
    "X_train = np.hstack([np.ones((X_train.shape[0], 1)), X_train])\n",
    "X_val = np.hstack([np.ones((X_val.shape[0], 1)), X_val])\n",
    "X_test = np.hstack([np.ones((X_test.shape[0], 1)), X_test])\n",
    "\n",
    "print(X_train[:5], '\\n\\n', y_train[:5])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d6132e0f",
   "metadata": {},
   "source": [
    "**2.1.5 Encode the categorical variables of the CarPrice dataset.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "be8a2941",
   "metadata": {},
   "outputs": [],
   "source": [
    "### Code here"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "41176bd0",
   "metadata": {},
   "source": [
    "**2.1.6 Plot the correlation matrix, and check if there is high correlation between the given numerical features (Threshold >=0.9). If yes, drop one from each pair of highly correlated features from the dataframe. Why is necessary to drop those columns before proceeding further?**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "40544c43",
   "metadata": {},
   "outputs": [],
   "source": [
    "### Code here"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "273227af",
   "metadata": {},
   "outputs": [],
   "source": [
    "#### Comment here"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "85ce946b",
   "metadata": {},
   "source": [
    "**2.1.7 Split the dataset into training (60%), validation (20%), and test (20%) sets. Use random_state = 0.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "0bb947cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "### Code here"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3b4ca085",
   "metadata": {},
   "source": [
    "**2.1.8 Standardize the columns in the feature matrices.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "bc869444",
   "metadata": {},
   "outputs": [],
   "source": [
    "### Code here"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6150f676",
   "metadata": {},
   "source": [
    "**2.1.9 Add a column of ones to the feature matrices for the bias term.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "3793f360",
   "metadata": {},
   "outputs": [],
   "source": [
    "### Code here"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5f1d84c3",
   "metadata": {},
   "source": [
    "At the end of this pre-processing, you should have the following vectors and matrices: \n",
    "- Syntheic dataset: X_train, X_val, X_test, y_train, y_val, y_test\n",
    "- Car Price Prediction dataset: car_price_X_train, car_price_X_val, car_price_X_test, car_price_y_train, car_price_y_val, car_price_y_test"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c5593921",
   "metadata": {},
   "source": [
    "### Implement Linear Regression"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f1a94f7a",
   "metadata": {},
   "source": [
    "Now, we can implement our linear regression model! Specifically, we will be implementing ridge regression, which is linear regression with L2 regularization. Given an (m x n) feature matrix $X$, an (m x 1) label vector $y$, and an (n x 1) weight vector $w$, the hypothesis function for linear regression is:\n",
    "\n",
    "$$\n",
    "y = X w\n",
    "$$\n",
    "\n",
    "Note that we can omit the bias term here because we have included a column of ones in our $X$ matrix, so the bias term is learned implicitly as a part of $w$. This will make our implementation easier.\n",
    "\n",
    "Our objective in linear regression is to learn the weights $w$ which best fit the data. This notion can be formalized as finding the optimal $w$ which minimizes the following loss function:\n",
    "\n",
    "$$\n",
    "\\min_{w} \\| X w - y \\|^2_2 + \\alpha \\| w \\|^2_2 \\\\\n",
    "$$\n",
    "\n",
    "This is the ridge regression loss function. The $\\| X w - y \\|^2_2$ term penalizes predictions $Xw$ which are not close to the label $y$. And the $\\alpha \\| w \\|^2_2$ penalizes large weight values, to favor a simpler, more generalizable model. The $\\alpha$ hyperparameter, known as the regularization parameter, is used to tune the complexity of the model - a higher $\\alpha$ results in smaller weights and lower complexity, and vice versa. Setting $\\alpha = 0$ gives us vanilla linear regression.\n",
    "\n",
    "Conveniently, ridge regression has a closed-form solution which gives us the optimal $w$ without having to do iterative methods such as gradient descent. The closed-form solution, known as the Normal Equations, is given by:\n",
    "\n",
    "$$\n",
    "w = (X^T X + \\alpha I)^{-1} X^T y\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "115d3d7c",
   "metadata": {},
   "source": [
    "**2.1.10 Implement a `LinearRegression` class with two methods: `train` and `predict`.**\n",
    "\n",
    "**Note: You may NOT use sklearn for this implementation. You may, however, use `np.linalg.solve` to find the closed-form solution. It is highly recommended that you vectorize your code.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "951d66dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "class LinearRegression():\n",
    "    '''\n",
    "    Linear regression model with L2-regularization (i.e. ridge regression).\n",
    "\n",
    "    Attributes\n",
    "    ----------\n",
    "    alpha: regularization parameter\n",
    "    w: (n x 1) weight vector\n",
    "    '''\n",
    "    \n",
    "    def __init__(self, alpha=0):\n",
    "        self.alpha = alpha\n",
    "        self.w = None\n",
    "\n",
    "    def train(self, X, y):\n",
    "        '''Trains model using ridge regression closed-form solution \n",
    "        (sets w to its optimal value).\n",
    "        \n",
    "        Parameters\n",
    "        ----------\n",
    "        X : (m x n) feature matrix\n",
    "        y: (m x 1) label vector\n",
    "        \n",
    "        Returns\n",
    "        -------\n",
    "        None\n",
    "        '''\n",
    "        ### Your code here\n",
    "        pass\n",
    "        \n",
    "    def predict(self, X):\n",
    "        '''Predicts on X using trained model.\n",
    "        \n",
    "        Parameters\n",
    "        ----------\n",
    "        X : (m x n) feature matrix\n",
    "        \n",
    "        Returns\n",
    "        -------\n",
    "        y_pred: (m x 1) prediction vector\n",
    "        '''\n",
    "        ### Your code here\n",
    "        pass\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "12b31e53",
   "metadata": {},
   "source": [
    "### Train, Evaluate, and Interpret LR Model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d07b1578",
   "metadata": {},
   "source": [
    "**2.1.11 Using your `LinearRegression` implementation above, train a vanilla linear regression model ($\\alpha = 0$) on (X_train, y_train) from the synthetic dataset. Use this trained model to predict on X_test. Report the first 3 and last 3 predictions on X_test, along with the actual labels in y_test.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "a2fb09b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "### Code here"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a6a99e9a",
   "metadata": {},
   "source": [
    "**2.1.12 Plot a scatter plot of y_test vs. X_test (just the non-ones column). Then, using the weights from the trained model above, plot the best-fit line for this data on the same figure.** \n",
    "\n",
    ">If your line goes through the data points, you have likely implemented the linear regression correctly!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "b9dcaacd",
   "metadata": {},
   "outputs": [],
   "source": [
    "### Code here"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1179de8f",
   "metadata": {},
   "source": [
    "**2.1.13 Train a linear regression model ($\\alpha = 0$) on the car price training data. Make predictions and report the $R^2$ score on the training, validation, and test sets. Report the first 3 and last 3 predictions on the test set, along with the actual labels.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "795919fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "### Code here"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "051c7530",
   "metadata": {},
   "source": [
    "**2.1.14 As a baseline model, use the mean of the training labels (car_price_y_train) as the prediction for all instances. Report the $R^2$ on the training, validation, and test sets using this baseline.** \n",
    "\n",
    ">This is a common baseline used in regression problems and tells you if your model is any good. Your linear regression $R^2$ should be much higher than these baseline $R^2$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "ffc6053f",
   "metadata": {},
   "outputs": [],
   "source": [
    "### Code here"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "539a93e2",
   "metadata": {},
   "source": [
    "**2.1.15 Interpret your model trained on the car price dataset using a bar chart of the model weights. Make sure to label the bars (x-axis) and don't forget the bias term!**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "090609c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "### Code here"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0540208a",
   "metadata": {},
   "source": [
    "**2.1.16 According to your model, which features are the greatest contributors to the car price?**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "f40e895a",
   "metadata": {},
   "outputs": [],
   "source": [
    "#### Comment here"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d3e0c8ca",
   "metadata": {},
   "source": [
    "### Hyperparameter Tuning ($\\alpha$)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4f305ddd",
   "metadata": {},
   "source": [
    "Now, let's do ridge regression and tune the $\\alpha$ regularization parameter on the car price dataset.\n",
    "\n",
    "**2.1.17 Sweep out values for $\\alpha$ using alphas = np.logspace(-5, 1, 20). Perform a grid search over these $\\alpha$ values, recording the training and validation $R^2$ for each $\\alpha$. A simple grid search is fine, no need for k-fold cross validation. Plot the training and validation $R^2$ as a function of $\\alpha$ on a single figure. Make sure to label the axes and the training and validation $R^2$ curves. Use a log scale for the x-axis.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "f2fc3193",
   "metadata": {},
   "outputs": [],
   "source": [
    "### Code here"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cfd376b2",
   "metadata": {},
   "source": [
    "**2.1.18 Explain your plot above. How do training and validation $R^2$ behave with decreasing model complexity (increasing $\\alpha$)?**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "f91e0b84",
   "metadata": {},
   "outputs": [],
   "source": [
    "#### Comment here"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "71ff765b",
   "metadata": {},
   "source": [
    "### 2.2 Logistic Regression"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "48a76fb4",
   "metadata": {},
   "source": [
    "In this part, we will be using a heart disease dataset for classification. \n",
    "\n",
    "The classification goal is to predict whether the patient has 10-year risk of future coronary heart disease (CHD).The dataset provides information about patients, over 4,000 records and 15 attributes.\n",
    "\n",
    "\n",
    "**Variables:**\n",
    "\n",
    "Each attribute is a potential risk factor. There are both demographic, behavioral and medical risk factors.\n",
    "\n",
    "Demographic:\n",
    "- Sex: male or female(Nominal)\n",
    "- Age: Age of the patient;(Continuous - Although the recorded ages have been truncated to whole numbers, the concept of age is continuous)\n",
    "\n",
    "Behavioral:\n",
    "- Current Smoker: whether or not the patient is a current smoker (Nominal)\n",
    "- Cigs Per Day: the number of cigarettes that the person smoked on average in one day.(can be considered continuous as one can have any number of cigarettes, even half a cigarette.)\n",
    "\n",
    "Medical( history):\n",
    "- BP Meds: whether or not the patient was on blood pressure medication (Nominal)\n",
    "- Prevalent Stroke: whether or not the patient had previously had a stroke (Nominal)\n",
    "- Prevalent Hyp: whether or not the patient was hypertensive (Nominal)\n",
    "- Diabetes: whether or not the patient had diabetes (Nominal)\n",
    "\n",
    "Medical(current):\n",
    "- Tot Chol: total cholesterol level (Continuous)\n",
    "- Sys BP: systolic blood pressure (Continuous)\n",
    "- Dia BP: diastolic blood pressure (Continuous)\n",
    "- BMI: Body Mass Index (Continuous)\n",
    "- Heart Rate: heart rate (Continuous - In medical research, variables such as heart rate though in fact discrete, yet are considered continuous because of large number of possible values.)\n",
    "- Glucose: glucose level (Continuous)\n",
    "\n",
    "Predict variable (desired target):\n",
    "- 10 year risk of coronary heart disease CHD (binary: “1”, means “Yes”, “0” means “No”)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "0c8d6353",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>male</th>\n",
       "      <th>age</th>\n",
       "      <th>education</th>\n",
       "      <th>currentSmoker</th>\n",
       "      <th>cigsPerDay</th>\n",
       "      <th>BPMeds</th>\n",
       "      <th>prevalentStroke</th>\n",
       "      <th>prevalentHyp</th>\n",
       "      <th>diabetes</th>\n",
       "      <th>totChol</th>\n",
       "      <th>sysBP</th>\n",
       "      <th>diaBP</th>\n",
       "      <th>BMI</th>\n",
       "      <th>heartRate</th>\n",
       "      <th>glucose</th>\n",
       "      <th>TenYearCHD</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>39</td>\n",
       "      <td>4.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>195.0</td>\n",
       "      <td>106.0</td>\n",
       "      <td>70.0</td>\n",
       "      <td>26.97</td>\n",
       "      <td>80.0</td>\n",
       "      <td>77.0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>46</td>\n",
       "      <td>2.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>250.0</td>\n",
       "      <td>121.0</td>\n",
       "      <td>81.0</td>\n",
       "      <td>28.73</td>\n",
       "      <td>95.0</td>\n",
       "      <td>76.0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1</td>\n",
       "      <td>48</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1</td>\n",
       "      <td>20.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>245.0</td>\n",
       "      <td>127.5</td>\n",
       "      <td>80.0</td>\n",
       "      <td>25.34</td>\n",
       "      <td>75.0</td>\n",
       "      <td>70.0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0</td>\n",
       "      <td>61</td>\n",
       "      <td>3.0</td>\n",
       "      <td>1</td>\n",
       "      <td>30.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>225.0</td>\n",
       "      <td>150.0</td>\n",
       "      <td>95.0</td>\n",
       "      <td>28.58</td>\n",
       "      <td>65.0</td>\n",
       "      <td>103.0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0</td>\n",
       "      <td>46</td>\n",
       "      <td>3.0</td>\n",
       "      <td>1</td>\n",
       "      <td>23.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>285.0</td>\n",
       "      <td>130.0</td>\n",
       "      <td>84.0</td>\n",
       "      <td>23.10</td>\n",
       "      <td>85.0</td>\n",
       "      <td>85.0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   male  age  education  currentSmoker  cigsPerDay  BPMeds  prevalentStroke  \\\n",
       "0     1   39        4.0              0         0.0     0.0                0   \n",
       "1     0   46        2.0              0         0.0     0.0                0   \n",
       "2     1   48        1.0              1        20.0     0.0                0   \n",
       "3     0   61        3.0              1        30.0     0.0                0   \n",
       "4     0   46        3.0              1        23.0     0.0                0   \n",
       "\n",
       "   prevalentHyp  diabetes  totChol  sysBP  diaBP    BMI  heartRate  glucose  \\\n",
       "0             0         0    195.0  106.0   70.0  26.97       80.0     77.0   \n",
       "1             0         0    250.0  121.0   81.0  28.73       95.0     76.0   \n",
       "2             0         0    245.0  127.5   80.0  25.34       75.0     70.0   \n",
       "3             1         0    225.0  150.0   95.0  28.58       65.0    103.0   \n",
       "4             0         0    285.0  130.0   84.0  23.10       85.0     85.0   \n",
       "\n",
       "   TenYearCHD  \n",
       "0           0  \n",
       "1           0  \n",
       "2           0  \n",
       "3           1  \n",
       "4           0  "
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "heart_disease_df = pd.read_csv('heart_disease.csv')\n",
    "heart_disease_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6905f676",
   "metadata": {
    "id": "XUnefT_j8mVN"
   },
   "source": [
    "#### Missing Value Analysis\n",
    "\n",
    "**2.2.1 Are there any missing values in the dataset? If so, what can be done about it? (Think if removing is an option?)**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "c3b245fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "### Code here"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "12f62920",
   "metadata": {},
   "outputs": [],
   "source": [
    "#### Comment here"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f4cbc470",
   "metadata": {
    "id": "IixTesFi6oyr"
   },
   "source": [
    "**2.2.2 Do you think that the distribution of labels is balanced? Why/why not? \n",
    "Hint: Find the probability of the different categories.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "0911b10e",
   "metadata": {},
   "outputs": [],
   "source": [
    "### Code here"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "a623c048",
   "metadata": {},
   "outputs": [],
   "source": [
    "#### Comment here"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9ecc3464",
   "metadata": {
    "id": "zzLzmgLvR_LH"
   },
   "source": [
    "**2.2.3 Plot the correlation matrix (first separate features and Y variable), and check if there is high correlation between the given numerical features (Threshold >=0.9). If yes, drop those highly correlated features from the dataframe.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "590bc3ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "### Code here"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "5574878d",
   "metadata": {},
   "outputs": [],
   "source": [
    "#### Comment here"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "feef9026",
   "metadata": {
    "id": "10Ss0nmX6oo9"
   },
   "source": [
    "**2.2.4 Apply the following pre-processing steps:**\n",
    "\n",
    "1. Convert the label from a Pandas series to a Numpy (m x 1) vector. If you don't do this, it may cause problems when implementing the logistic regression model.\n",
    "2. Split the dataset into training (60%), validation (20%), and test (20%) sets.\n",
    "3. Standardize the columns in the feature matrices. To avoid information leakage, learn the standardization parameters from training, and then apply training, validation and test dataset.\n",
    "4. Add a column of ones to the feature matrices of train, validation and test dataset. This is a common trick so that we can learn a coefficient for the bias term of a linear model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "a22bd4f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "### Code here"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8f7594ae",
   "metadata": {
    "id": "fahb5S5B6omA"
   },
   "source": [
    "### Implement Logistic Regression\n",
    "\n",
    "We will now implement logistic regression with L2 regularization. Given an (m x n) feature matrix $X$, an (m x 1) label vector $y$, and an (n x 1) weight vector $w$, the hypothesis function for logistic regression is:\n",
    "\n",
    "$$\n",
    "y = \\sigma(X w)\n",
    "$$\n",
    "\n",
    "where $\\sigma(x) = \\frac{1}{1 + e^{-x}}$, i.e. the sigmoid function. This function scales the prediction to be a probability between 0 and 1, and can then be thresholded to get a discrete class prediction.\n",
    "\n",
    "Just as with linear regression, our objective in logistic regression is to learn the weights $𝑤$ which best fit the data. For L2-regularized logistic regression, we find an optimal $w$ to minimize the following loss function:\n",
    "\n",
    "$$\n",
    "\\min_{w} \\ -y^T \\ \\text{log}(\\sigma(Xw)) \\ - \\  (\\mathbf{1} - y)^T \\ \\text{log}(\\mathbf{1} - \\sigma(Xw)) \\ + \\ \\alpha \\| w \\|^2_2 \\\\\n",
    "$$\n",
    "\n",
    "Unlike linear regression, however, logistic regression has no closed-form solution for the optimal $w$. So, we will use gradient descent to find the optimal $w$. The (n x 1) gradient vector $g$ for the loss function above is:\n",
    "\n",
    "$$\n",
    "g = X^T \\Big(\\sigma(Xw) - y\\Big) + 2 \\alpha w\n",
    "$$\n",
    "\n",
    "Below is pseudocode for gradient descent to find the optimal $w$. You should first initialize $w$ (e.g. to a (n x 1) zero vector). Then, for some number of epochs $t$, you should update $w$ with $w - \\eta g $, where $\\eta$ is the learning rate and $g$ is the gradient. You can learn more about gradient descent [here](https://www.coursera.org/lecture/machine-learning/gradient-descent-8SpIM).\n",
    "\n",
    "> $w = \\mathbf{0}$\n",
    "> \n",
    "> $\\text{for } i = 1, 2, ..., t$\n",
    ">\n",
    "> $\\quad \\quad w = w - \\eta g $\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7306eff4",
   "metadata": {
    "id": "qaSAcopq6ojI"
   },
   "source": [
    "**A LogisticRegression class with five methods: train, predict, calculate_loss, calculate_gradient, and calculate_sigmoid has been implemented for you below.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "ae48fbc5",
   "metadata": {
    "id": "st4bG7WtI0qI"
   },
   "outputs": [],
   "source": [
    "class LogisticRegression():\n",
    "    '''\n",
    "    Logistic regression model with L2 regularization.\n",
    "\n",
    "    Attributes\n",
    "    ----------\n",
    "    alpha: regularization parameter\n",
    "    t: number of epochs to run gradient descent\n",
    "    eta: learning rate for gradient descent\n",
    "    w: (n x 1) weight vector\n",
    "    '''\n",
    "    \n",
    "    def __init__(self, alpha=0, t=100, eta=1e-3):\n",
    "        self.alpha = alpha\n",
    "        self.t = t\n",
    "        self.eta = eta\n",
    "        self.w = None\n",
    "\n",
    "    def train(self, X, y):\n",
    "        '''Trains logistic regression model using gradient descent \n",
    "        (sets w to its optimal value).\n",
    "        \n",
    "        Parameters\n",
    "        ----------\n",
    "        X : (m x n) feature matrix\n",
    "        y: (m x 1) label vector\n",
    "        \n",
    "        Returns\n",
    "        -------\n",
    "        losses: (t x 1) vector of losses at each epoch of gradient descent\n",
    "        '''\n",
    "        \n",
    "        loss = list()\n",
    "        self.w = np.zeros((X.shape[1],1))\n",
    "        for i in range(self.t): \n",
    "            self.w = self.w - (self.eta * self.calculate_gradient(X, y))\n",
    "            loss.append(self.calculate_loss(X, y))\n",
    "        return loss\n",
    "        \n",
    "    def predict(self, X):\n",
    "        '''Predicts on X using trained model. Make sure to threshold \n",
    "        the predicted probability to return a 0 or 1 prediction.\n",
    "        \n",
    "        Parameters\n",
    "        ----------\n",
    "        X : (m x n) feature matrix\n",
    "        \n",
    "        Returns\n",
    "        -------\n",
    "        y_pred: (m x 1) 0/1 prediction vector\n",
    "        '''\n",
    "        y_pred = self.calculate_sigmoid(X.dot(self.w))\n",
    "        y_pred[y_pred >= 0.5] = 1\n",
    "        y_pred[y_pred < 0.5] = 0\n",
    "        return y_pred\n",
    "    \n",
    "    def calculate_loss(self, X, y):\n",
    "        '''Calculates the logistic regression loss using X, y, w, \n",
    "        and alpha. Useful as a helper function for train().\n",
    "        \n",
    "        Parameters\n",
    "        ----------\n",
    "        X : (m x n) feature matrix\n",
    "        y: (m x 1) label vector\n",
    "        \n",
    "        Returns\n",
    "        -------\n",
    "        loss: (scalar) logistic regression loss\n",
    "        '''\n",
    "        return -y.T.dot(np.log(self.calculate_sigmoid(X.dot(self.w)))) - (1-y).T.dot(np.log(1-self.calculate_sigmoid(X.dot(self.w)))) + self.alpha*np.linalg.norm(self.w, ord=2)**2\n",
    "    \n",
    "    def calculate_gradient(self, X, y):\n",
    "        '''Calculates the gradient of the logistic regression loss \n",
    "        using X, y, w, and alpha. Useful as a helper function \n",
    "        for train().\n",
    "        \n",
    "        Parameters\n",
    "        ----------\n",
    "        X : (m x n) feature matrix\n",
    "        y: (m x 1) label vector\n",
    "        \n",
    "        Returns\n",
    "        -------\n",
    "        gradient: (n x 1) gradient vector for logistic regression loss\n",
    "        '''\n",
    "        return X.T.dot(self.calculate_sigmoid( X.dot(self.w)) - y) + 2*self.alpha*self.w        \n",
    "            \n",
    "    \n",
    "    def calculate_sigmoid(self, x):\n",
    "        '''Calculates the sigmoid function on each element in vector x. \n",
    "        Useful as a helper function for predict(), calculate_loss(), \n",
    "        and calculate_gradient().\n",
    "        \n",
    "        Parameters\n",
    "        ----------\n",
    "        x: (m x 1) vector\n",
    "        \n",
    "        Returns\n",
    "        -------\n",
    "        sigmoid_x: (m x 1) vector of sigmoid on each element in x\n",
    "        '''\n",
    "        return (1)/(1 + np.exp(-x.astype('float')))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a1bbc9b1",
   "metadata": {
    "id": "AX7qSyEg6oeG"
   },
   "source": [
    "**2.2.5 Plot Loss over Epoch and Search the space randomly to find best hyperparameters.**\n",
    "\n",
    "A: Using your implementation above, train a logistic regression model **(alpha=0, t=100, eta=1e-3)** on the voice recognition training data. Plot the training loss over epochs. Make sure to label your axes. You should see the loss decreasing and start to converge. \n",
    "\n",
    "B: Using **alpha between (0,1), eta between(0, 0.001) and t between (0, 100)**, find the best hyperparameters for LogisticRegression. You can randomly search the space 20 times to find the best hyperparameters.\n",
    "\n",
    "C. Compare accuracy on the test dataset for both the scenarios."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "136abd89",
   "metadata": {},
   "outputs": [],
   "source": [
    "### Code here"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bd3dc6f3",
   "metadata": {},
   "source": [
    "**2.2.6 Do you think the model is performing well keeping the class distribution in mind?**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "bca535e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "#### Comment here"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e8807c92",
   "metadata": {},
   "source": [
    "> We will look into different evaluation metrics in Lecture 5 that will help us with such imbalanced datasets. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2f9cfb46",
   "metadata": {
    "id": "cfFGXYlk6oUj"
   },
   "source": [
    "### Feature Importance\n",
    "\n",
    "**2.2.7 Interpret your trained model using a bar chart of the model weights. Make sure to label the bars (x-axis) and don't forget the bias term!**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "639a8007",
   "metadata": {},
   "outputs": [],
   "source": [
    "### Code here"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "e2165aaf",
   "metadata": {},
   "outputs": [],
   "source": [
    "#### Comment here"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "23b3ce58",
   "metadata": {
    "id": "6F-hltTTJeF2"
   },
   "source": [
    "\n",
    "# **Part 3: Support Vector Machines**\n",
    "\n",
    "In this part, we will be using support vector machines for classification on the heart disease dataset."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "14a0701f",
   "metadata": {
    "id": "MJZ0qe_ZJsVu"
   },
   "source": [
    "### Train Primal SVM\n",
    "**3.1 Train a primal SVM (with default parameters) on the heart disease dataset. Make predictions and report the accuracy on the training, validation, and test sets.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "88f7fcab",
   "metadata": {},
   "outputs": [],
   "source": [
    "### Code here"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "697691a0",
   "metadata": {
    "id": "ri5ZXq5kLo75"
   },
   "source": [
    "### Train Dual SVM\n",
    "**3.2 Train a dual SVM (with default parameters) on the heart disease dataset. Make predictions and report the accuracy on the training, validation, and test sets.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "140962d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "### Code here"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
